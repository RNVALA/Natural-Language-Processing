{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAosyNiay4Bk"
   },
   "source": [
    "##### how can we capture similarity between two words?\n",
    "- so wee can make vector and we can do similarity\n",
    "- issue with word2vec -Fixed embeddings\n",
    "     1. he didn't receive fair treatment\n",
    "     2. Fun fair in new york city this summer\n",
    "here fun meaning is different but wordembedding will fiixed\n",
    "#####  to overcome this issue \n",
    "#Bert can generate contextualized embeding\n",
    "#Bert can generate embedding for entire sentence=>all sentence will have only one vector\n",
    "#### There are two version of Bert\n",
    "1. Bert Base-->comparable in size to the openai transformer in order to compare performance.12encoded layer\n",
    "2. Bert Large-->A ridiculously huge model which achieved the state of the art results reported in the paper.24 encoded layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwrKsntty_bY"
   },
   "source": [
    "- bert  is trained on 2500 million words of wikipidia and 800m words of other books\n",
    "  - How was it trained\n",
    "    - Massed Language model\n",
    "    - Next sentance prediction\n",
    "- google search is powered by bert\n",
    "- bert full form-->bidirectional encoder representation from transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TI-B7JqFzEbW",
    "outputId": "2f4f4f06-9058-43df-ac5f-a25a868349d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow_text\n",
      "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.13.0)\n",
      "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.54.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.8)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.22.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.2.2)\n",
      "Installing collected packages: tensorflow_text\n",
      "Successfully installed tensorflow_text-2.12.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade tensorflow\n",
    "!pip install tensorflow_text\n",
    "# !pip install --upgrade protobuf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "#TensorFlow Text is a powerful library that can be used for a variety of text processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7IJJTuizPlR"
   },
   "outputs": [],
   "source": [
    "preprocessing_url=(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "#above URL that points to a tensorflow hub module that can be used to preprocess text for use with bert model\n",
    "encoder_url='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3idQ4_ozULS"
   },
   "outputs": [],
   "source": [
    "#to create hub layer and pass your preprocess url\n",
    "bert_preprocess_model=hub.KerasLayer(preprocessing_url)\n",
    "#it creates a tensorflow hub layer that can be used to preprocess text data for use with bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4YKSKscz18B",
    "outputId": "81c5071c-2625-4dc1-803c-6f03e17a9268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_word_ids', 'input_type_ids', 'input_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test=['nice movie indeed','I love python programing']\n",
    "text_preprocess=bert_preprocess_model(text_test)\n",
    "text_preprocess.keys()\n",
    "#here input_word_ids: A tensor that contain the input text data after it has been tokenized and convertrd to integer ids.\n",
    "#input_mask:A tensor that indicates which tokens in the input text are padding tookens\n",
    "#input_type_ids:A tensor that indicates the paer of speech tag for eacch token iin the input text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmR1DIXt0TQV",
    "outputId": "479fd162-ac5d-4967-bfb9-7baa6fb1d946"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocess['input_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9n7DXbE0e0_"
   },
   "outputs": [],
   "source": [
    "# we can see that first sentance is having 3 words and mask is 05 \n",
    "#cls nice movie indeed sep\n",
    "#so it print cls in begbbning and sep in last thay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoNpNk1W1KD-",
    "outputId": "3ea50c59-e92a-4187-8c7e-4f869a9e1ed5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[  101,  3835,  3185,  5262,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  1045,  2293, 18750,  2565,  2075,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocess['input_word_ids']\n",
    "# cls nice movie indeed SEP\n",
    "#cls is always fixed 101 ans seperation 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SE8Y9C2k1Ous",
    "outputId": "e68799b4-3eb8-443c-b9c8-d84f5ca2a0b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['default', 'pooled_output', 'encoder_outputs', 'sequence_output'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model=hub.KerasLayer(encoder_url)\n",
    "bert_results=bert_model(text_preprocess)\n",
    "bert_results.keys()\n",
    "#it gives output as dict_keys(['default', 'pooled_output', 'encoder_outputs', 'sequence_output'])\n",
    "# The Default key contains a tensor that represents the output of the bert model.This tensor can be used for a verity of tasks,such as text classification,sentiment analysis,and question answerig\n",
    "#The pooled_output key contains a tensor that represents the pooled output of the bert model.The pooled output is a single vector that represent the entire text. This vector can be used fot tasjs that require a global representation of the input text,such as text summarization and question answering.\n",
    "\n",
    "#Encoder_ouptput key contains a tensor that represents the encoder outputs of the bert model.\n",
    "#The encoder outputs are sequence of vector that represent the input text at different levels of granularity.\n",
    "#These vectors can be used for tasks that require a more detailed understanding of the input text, such as named entity recognition and nlp inference\n",
    "\n",
    "\n",
    "#Sequence_output\n",
    "#sequence_output key contains a tensor that represent the sequencce output of the BERT model.The Sequence output is a sequence of vectors that represent the input text at the word level.\n",
    "#These vectors can be used for tasks that require afine grained understanding of the input text,such as part-of-speech tagging and semantic role labeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ov-iMyM71spE",
    "outputId": "036a06b7-718e-44aa-9c68-b7bf2f1ec19c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.791774  , -0.21411902,  0.49769545, ...,  0.24465242,\n",
       "        -0.47334465,  0.8175868 ],\n",
       "       [-0.8998869 , -0.5022243 , -0.80149144, ..., -0.602643  ,\n",
       "        -0.69911593,  0.9073097 ]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results['pooled_output']#the embedding vector size is 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eq7m3F1F2TA7",
    "outputId": "b3468394-0874-4986-a271-fd5a57ef7e70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.07292041,  0.08567816,  0.14476837, ..., -0.0967708 ,\n",
       "          0.08722115,  0.07711103],\n",
       "        [ 0.17839377, -0.19006082,  0.50349444, ..., -0.05869827,\n",
       "          0.32717147, -0.15578502],\n",
       "        [ 0.18701456, -0.4338877 , -0.48875162, ..., -0.1550281 ,\n",
       "          0.00145131, -0.24470958],\n",
       "        ...,\n",
       "        [ 0.12083077,  0.12884237,  0.46453524, ...,  0.07375517,\n",
       "          0.17441934,  0.16522089],\n",
       "        [ 0.07967839, -0.01190699,  0.5022542 , ...,  0.13777757,\n",
       "          0.21002182,  0.0062461 ],\n",
       "        [-0.07212706, -0.2830349 ,  0.5903332 , ...,  0.47551912,\n",
       "          0.1666846 , -0.08920319]],\n",
       "\n",
       "       [[ 0.04060526,  0.32671487, -0.06604554, ..., -0.11767737,\n",
       "          0.20420273,  0.6038268 ],\n",
       "        [ 0.42078924,  0.38885808, -0.19671586, ..., -0.25060526,\n",
       "          0.29196504,  0.34592384],\n",
       "        [ 1.0496668 ,  1.0857546 ,  0.4954928 , ...,  0.31622717,\n",
       "          0.27401122,  0.26547572],\n",
       "        ...,\n",
       "        [ 0.25059482,  0.14957339,  0.4347987 , ...,  0.21716337,\n",
       "         -0.1832553 ,  0.26310837],\n",
       "        [ 0.13218684,  0.16939896,  0.36954308, ...,  0.10122067,\n",
       "         -0.18612483,  0.23879418],\n",
       "        [ 0.10344119,  0.1208593 ,  0.3779924 , ...,  0.14138368,\n",
       "         -0.22387391,  0.23316574]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#individual word embeddinng vector\n",
    "bert_results['sequence_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix-1TXQT22wu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
