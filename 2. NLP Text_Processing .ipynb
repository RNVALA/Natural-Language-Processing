{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVgUfAcKJzgk"
   },
   "source": [
    "# **NLP: Text Processing**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEmstXz8BGCm"
   },
   "source": [
    "# **Processing Text Data with NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nQqGYHgBIuO"
   },
   "source": [
    "-- Italian recipes data\n",
    "\n",
    "- Data set of Italian recipes from https://www.gutenberg.org/ebooks/24407 (public domain)\n",
    "\n",
    "- The txt format of this has been split into multiple files, one recipe per file.\n",
    "\n",
    "- The data can be found in /recipes/{1, 2, ..., 220}.txt\n",
    "\n",
    "- There are 220 recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1682051617538,
     "user": {
      "displayName": "TANVI PATEL",
      "userId": "00246378902842286429"
     },
     "user_tz": -330
    },
    "id": "SpqApw3_CMTg",
    "outputId": "cb31da72-9700-4469-cb6f-112fc492512b"
   },
   "outputs": [],
   "source": [
    "#!unzip recipes.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ETvgLQN5DV1x"
   },
   "outputs": [],
   "source": [
    "#importing the dataset of recepes files\n",
    "import os\n",
    "\n",
    "data_folder = r'recipes/recipes/'\n",
    "all_recipe_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hpYyVRqC6Ucf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_recipe_files)#it will print the total length of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7wKwZQe3XtDJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recipes/recipes/1.txt',\n",
       " 'recipes/recipes/10.txt',\n",
       " 'recipes/recipes/100.txt',\n",
       " 'recipes/recipes/101.txt',\n",
       " 'recipes/recipes/102.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_recipe_files[0:5]#it will print the first 05 recepies file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iPmuqhyO6TEb"
   },
   "outputs": [],
   "source": [
    "documents = {}#crearting the dictionary\n",
    "\n",
    "for recipe_fname in all_recipe_files:\n",
    "    bname = os.path.basename(recipe_fname)\n",
    "\n",
    "    recipe_number = os.path.splitext(bname)[0]\n",
    "    \n",
    "    with open(recipe_fname, 'r') as f:\n",
    "        documents[recipe_number] = f.read()\n",
    "#it creates the a python dictionary called documents,then iterates over a list of file names stored in the 'all_racipe_files' variable\n",
    "#for each file name, the code extracts the base file name(i.e name without directory path or file extension)\n",
    "#and store basename in bname variable\n",
    "\n",
    "#then extract the recipe number from the base file name using 'os.path.splitext'\n",
    "#it stored in receipe_no\n",
    "\n",
    "#then it reads the contents of each recipe file in all_recipe_files, stores the contents in a dictionary called documents,\n",
    "#and uses the recipe number as the key to access the corresponding recipe in the dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fIRiDWQU6dRb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1',\n",
       "  '\\nBROTH OR SOUP STOCK\\n\\n(Brodo)\\n\\nTo obtain good broth the meat must be put in cold water, and then\\nallowed to boil slowly. Add to the meat some pieces of bones and \"soup\\ngreens\" as, for instance, celery, carrots and parsley. To give a brown\\ncolor to the broth, some sugar, first browned at the fire, then diluted\\nin cold water, may be added.\\n\\nWhile it is not considered that the broth has much nutritive power, it\\nis excellent to promote the digestion. Nearly all the Italian soups are\\nmade on a basis of broth.\\n\\nA good recipe for substantial broth to be used for invalids is the\\nfollowing: Cut some beef in thin slices and place them in a large\\nsaucepan; add some salt. Pour cold water upon them, so that they are\\nentirely covered. Cover the saucepan so that it is hermetically closed\\nand place on the cover a receptacle containing water, which must be\\nconstantly renewed. Keep on a low fire for six hours, then on a strong\\nfire for ten minutes. Strain the liquid in cheese cloth.\\n\\nThe soup stock, besides being used for soups, is a necessary ingredient\\nin hundreds of Italian dishes.\\n\\n\\n'),\n",
       " ('10',\n",
       "  '\\nRAVIOLI\\n\\nPut on the bread board about two pounds of flour in a heap; make a\\nhollow in the middle and put in it a piece of butter, three egg-yolks,\\nsalt and three or four tablespoonfuls of lukewarm water. Make a paste\\nand knead it well, then let it stand for an hour, wrapped or covered\\nwith a linen cloth. Then spread the paste to a thin sheet, as thin as a\\nten-cent piece.\\n\\nChop and grind pieces of roast or boiled chicken meat: add to it an\\nequal part of marrow from the bones of beef and pieces of brains, three\\nyolks, some crumbs of bread soaked in milk or broth and some grated\\ncheese (Parmesan or Swiss). Rub through a sieve and make little balls as\\nbig as a hazel-nut, which are to be placed at equal distances (a little\\nmore than an inch) in a line over the sheet of paste.\\n\\nBeat a whole egg and pass it over the paste with a brush all around the\\nlittle balls. Cover these with another sheet of paste, press down the\\nintervals between each ball, and then separate each section from the\\nother with a knife. Moisten the edges of each section with the finger\\ndipped in cold water, to make them stick together, and press them down\\nwith the fingers or the prongs of a fork. Then put to boil in water\\nseasoned with salt or, better still, in broth. The ravioli are then to\\nbe served hot seasoned with cheese and butter or with brown stock or\\ntomato sauce.\\n\\n\\n'),\n",
       " ('100',\n",
       "  '\\nFRIED EGG-PLANTS\\n\\n(Melanzane fritte)\\n\\nEgg-plant or, as they are also called, mad-apples are an excellent\\nvegetable which may be used as dressing or as a dish by itself. Small or\\nmiddle-sized egg-plants are to be preferred, as the big ones have\\nsometimes a slightly bitter taste.\\n\\nRemove the skin, cut into cubes, salt and leave them in a plate for a\\nfew hours. Then wipe them to remove the juice that they have thrown out,\\ndip in flour and fry in oil.\\n\\n\\n'),\n",
       " ('101',\n",
       "  '\\nSTEWED EGG-PLANTS\\n\\n(Melanzane in umido)\\n\\nRemove the skin, cut them into cubes and place on the fire with a piece\\nof butter. When this is all absorbed, complete the cooking with tomato\\nsauce (No. 12).\\n\\n\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(documents.items())[:4]\n",
    "#it will give list of first four item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N53-Zz3q6b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 220\n",
      "Corpus size (char): 161170\n"
     ]
    }
   ],
   "source": [
    "corpus_all_in_one = ' '.join([doc for doc in documents.values()])\n",
    "#above line is used to convert into string\n",
    "\n",
    "print(\"Number of docs: {}\".format(len(documents)))\n",
    "print(\"Corpus size (char): {}\".format(len(corpus_all_in_one)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2ffTVNCT6lUd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBROTH OR SOUP STOCK\\n\\n(Brodo)\\n\\nTo obtain good broth the meat must be put in cold water, and then\\nallowed to boil slowly. Add to the meat some pieces of bones and \"soup\\ngreens\" as, for instance, celery, carrots and parsley. To give a brown\\ncolor to the broth, some sugar, first browned at the fire, then diluted\\nin cold water, may be added.\\n\\nWhile it is not considered that the broth has much nutritive power, it\\nis excellent to promote the digestion. Nearly all the Italian soups are\\nmade on a basis of broth.\\n\\nA good recipe for substantial broth to be used for invalids is the\\nfollowing: Cut some beef in thin slices and place them in a large\\nsaucepan; add some salt. Pour cold water upon them, so that they are\\nentirely covered. Cover the saucepan so that it is hermetically closed\\nand place on the cover a receptacle containing water, which must be\\nconstantly renewed. Keep on a low fire for six hours, then on a strong\\nfire for ten minutes. Strain the liquid in cheese cloth.\\n\\nThe soup stock, bes'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_all_in_one[0:1000]\n",
    "#it will print the first1000 strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KIsluqpDsnW"
   },
   "source": [
    "## **Tokenisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOftZAeVDvjF"
   },
   "source": [
    "-- Tokenisation is the process of splitting a raw string into a list of tokens\n",
    "\n",
    "-- What is a token? We're interested in meaningful units of text\n",
    "\n",
    "- Words\n",
    "- Phrases\n",
    "- Punctuation\n",
    "- Numbers\n",
    "- Dates\n",
    "- Currencies\n",
    "- Hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JArsoq_iEAcF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ronak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CPShDqbgdCHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'awesome', '!', 'It', 'is', 'very', 'easy', 'to', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLP is awesome! It is very easy to learn.\"\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sSgHoB6WdMEb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MuFfedllBCD5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 33719\n"
     ]
    }
   ],
   "source": [
    "# Returns the list of syllables of words\n",
    "\n",
    "all_tokens = [t for t in word_tokenize(corpus_all_in_one)]\n",
    "\n",
    "print(\"Total number of tokens: {}\".format(len(all_tokens)))\n",
    "#it will print the length of token from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eFJDEvvh6uOA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BROTH', 'OR', 'SOUP', 'STOCK', '(', 'Brodo', ')', 'To', 'obtain', 'good']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K27M_UHEH8W"
   },
   "source": [
    "## **Counting Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je2SqQIOEKWN"
   },
   "source": [
    "-- Simple word count using collections.Counter\n",
    "\n",
    "-- We are interested in finding:\n",
    "\n",
    "- how many times a word occurs across the whole corpus (total number of occurrences)\n",
    "- in how many documents a word occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0jXwhdbzEONN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1933),\n",
       " (',', 1726),\n",
       " ('.', 1568),\n",
       " ('and', 1435),\n",
       " ('a', 1076),\n",
       " ('of', 988),\n",
       " ('in', 811),\n",
       " ('with', 726),\n",
       " ('it', 537),\n",
       " ('to', 452),\n",
       " ('or', 389),\n",
       " ('is', 337),\n",
       " ('(', 295),\n",
       " (')', 295),\n",
       " ('be', 266),\n",
       " ('them', 248),\n",
       " ('butter', 231),\n",
       " ('on', 220),\n",
       " ('water', 205),\n",
       " ('little', 198)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_term_frequency =Counter(all_tokens)\n",
    "\n",
    "total_term_frequency.most_common(20)\n",
    "#it will print the most common occuring or repeating word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vhlmbpzEWP9"
   },
   "source": [
    "## **Stop-words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viW5TqWbEaTt"
   },
   "source": [
    "-- We notice that some of the most common words above are not very interesting.\n",
    "\n",
    "-- These words are called stop-words, and they don't provide any particular meaning in isolation (articles, conjunctions, pronouns, etc.)\n",
    "\n",
    "-- Notice:\n",
    "\n",
    "- there is no \"universal\" list of stop-words\n",
    "- removing stop-words can be useful or damaging depending on the application\n",
    "- e.g. if you remove stop-words, what do you do with \"The Who\", \"to be or not to be\" and similar phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "S_Nt8rNbEjwV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ronak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')#downloading the nltk stopword module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "t9a14JsAEfE3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "------------\n",
      "179\n",
      "-----------\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "print(stopwords.words('english'))#it will print all the stop words in nltk stopword module\n",
    "print(\"------------\")\n",
    "print(len(stopwords.words('english')))#it will print the length of the nltk stopword\n",
    "print('-----------')\n",
    "print(string.punctuation)#it will print punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "khrUTTyjEpcH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('butter', 231),\n",
       " ('water', 205),\n",
       " ('little', 198),\n",
       " ('put', 197),\n",
       " ('one', 186),\n",
       " ('salt', 185),\n",
       " ('fire', 169),\n",
       " ('half', 169),\n",
       " ('two', 157),\n",
       " ('When', 132),\n",
       " ('sauce', 128),\n",
       " ('pepper', 128),\n",
       " ('add', 125),\n",
       " ('cut', 125),\n",
       " ('flour', 116),\n",
       " ('piece', 116),\n",
       " ('The', 111),\n",
       " ('sugar', 100),\n",
       " ('saucepan', 100),\n",
       " ('oil', 99)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = stopwords.words('english') + list(string.punctuation)\n",
    "#above line stop words from nltk stopword module and it also generates punctuation from string module\n",
    "#and store in the stop_list\n",
    "\n",
    "tokens_no_stop = [token for token in all_tokens\n",
    "                 if token not in stop_list]\n",
    "#above line will  keep token that are not in the 'stop_list' variable\n",
    "\n",
    "total_term_frequency_no_stop = Counter(tokens_no_stop)\n",
    "#it will calculate the repeating word and then stored in a variable \n",
    "total_term_frequency_no_stop.most_common(20)\n",
    "#it will print the 20 most common occuring token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "96MVu1fcO-yP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('salt', 142),\n",
       " ('butter', 137),\n",
       " ('put', 126),\n",
       " ('water', 125),\n",
       " ('one', 117),\n",
       " ('fire', 115),\n",
       " ('little', 107),\n",
       " ('pepper', 106),\n",
       " ('two', 105),\n",
       " ('half', 105),\n",
       " ('When', 101),\n",
       " ('cut', 94),\n",
       " ('piece', 87),\n",
       " ('add', 83),\n",
       " ('saucepan', 81),\n",
       " ('oil', 78),\n",
       " ('sauce', 75),\n",
       " ('flour', 74),\n",
       " ('The', 72),\n",
       " ('Put', 69)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find in how many documents a word occurs, after removing stopwords\n",
    "\n",
    "document_frequency = Counter()\n",
    "#above line creates an empty counter object  to store the frequency of each unique token across all documets\n",
    "\n",
    "for recepe_number, content in documents.items():\n",
    "    tokens = word_tokenize(content)\n",
    "    tokens = [token for token in tokens if token not in stop_list]\n",
    "    unique_tokens = set(tokens)\n",
    "    document_frequency.update(unique_tokens)\n",
    "    \n",
    "document_frequency.most_common(20)\n",
    "#In summary, this code calculates the document frequency of each word in a collection of documents after removing stop words,\n",
    "#and prints the 20 most common words across all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRKpfMOYE0Jd"
   },
   "source": [
    "## **Text Normalisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zPqGyN_E28e"
   },
   "source": [
    "-- Replacing tokens with a canonical form, so we can group together different spelling/variations of the same word\n",
    "\n",
    "-- Examples:\n",
    "\n",
    "- lowercasing\n",
    "-  stemming\n",
    "-  American-to-British mapping\n",
    "- synonym mapping\n",
    "\n",
    "-- **Stemming** is the process of reducing a word to its base/root form, called stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcqYRFstwc3d"
   },
   "source": [
    "## **Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ijdKUAif9qmu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " ',',\n",
       " 'you',\n",
       " 'have',\n",
       " 'build',\n",
       " 'a',\n",
       " 'veri',\n",
       " 'good',\n",
       " 'applic',\n",
       " 'and',\n",
       " 'i',\n",
       " 'love',\n",
       " 'use',\n",
       " 'thi',\n",
       " 'product',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "sentence = \"Hello, You have build a very good application and I love using this product.\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words_stem = [ps.stem(word) for word in words]\n",
    "words_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xUt_nrFiE98_"
   },
   "outputs": [],
   "source": [
    "# Stemmed Recipe Corpus\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# all_tokens_lower = [t.lower(t for t in all_tokens)]\n",
    "\n",
    "# tokens_normalised = [stemmer.stem(t) for t in all_tokens_lower if t not in stop_list]\n",
    "\n",
    "# total_term_frequency_normalised = Counter(tokens_normalised)\n",
    "\n",
    "# total_term_frequency_normalised.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3TtIJmHwaG4"
   },
   "source": [
    "## **Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t2GTSXy9fpq"
   },
   "source": [
    "**Lemmatization** is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "\n",
    "Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vzg7-1gn--8"
   },
   "source": [
    "Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrpR9NIv-gNR"
   },
   "source": [
    "**Wordnet** is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words, synonym or antonym. One can define it as a semantically oriented dictionary of English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RuIFQnDq9XQv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ronak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aRA3Uax19TgM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY4YaRxgmP3t"
   },
   "source": [
    "- **Stemming v/s Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "8JLt3JqbmR0L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for studies is studi\n",
      "Stemming for studying is studi\n",
      "Stemming for cries is cri\n",
      "Stemming for cry is cri\n",
      "--------------------------------------------------\n",
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "#Stemming involves removing the suffix of a word to obtain its root form, also known as the stem. For example, the stem of the words \"running\", \"runner\", and \"runs\" is \"run\". \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps=PorterStemmer()\n",
    "lem=WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "  print(\"Stemming for {} is {}\".format(w, ps.stem(w)))\n",
    "print(\"--------------------------------------------------\")\n",
    "for w in tokenization:\n",
    "  print(\"Lemma for {} is {}\".format(w, lem.lemmatize(w)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "5ejONEe7heDN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('put', 276),\n",
       " ('butter', 243),\n",
       " ('piece', 211),\n",
       " ('one', 210),\n",
       " ('water', 209),\n",
       " ('little', 198),\n",
       " ('salt', 197),\n",
       " ('half', 173),\n",
       " ('fire', 169),\n",
       " ('cut', 166),\n",
       " ('egg', 163),\n",
       " ('two', 162),\n",
       " ('add', 160),\n",
       " ('sauce', 152),\n",
       " ('pepper', 130),\n",
       " ('flour', 123),\n",
       " ('sugar', 116),\n",
       " ('brown', 105),\n",
       " ('saucepan', 101),\n",
       " ('onion', 101)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatized Recipe Corpus\n",
    "#Lemmatization, on the other hand, involves reducing words to their base form or lemma by taking into account their context and part of speech. For example, the lemma of the words \"running\", \"runner\", and \"runs\" is \"run\". \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "all_tokens_lower = [t.lower() for t in all_tokens]\n",
    "\n",
    "tokens_lemmatized = [lemmatizer.lemmatize(t) for t in all_tokens_lower\n",
    "                    if t not in stop_list]\n",
    "\n",
    "total_term_frequency_normalised = Counter(tokens_lemmatized)\n",
    "\n",
    "total_term_frequency_normalised.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7y9LridFBVd"
   },
   "source": [
    "## **n-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p0EaLAPFECt"
   },
   "source": [
    "-- When we are interested in phrases rather than single terms, we can look into n-grams\n",
    "\n",
    "-- An n-gram is a sequence of n adjacent terms.\n",
    "\n",
    "-- Commonly used n-grams include bigrams (n=2) and trigrams (n=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "QTvF2XUpFIbl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('salt', 'pepper'), 106),\n",
       " (('piece', 'butter'), 82),\n",
       " (('grated', 'cheese'), 55),\n",
       " (('put', 'fire'), 47),\n",
       " (('season', 'salt'), 46),\n",
       " (('bread', 'crumb'), 41),\n",
       " (('tomato', 'sauce'), 36),\n",
       " (('complete', 'cooking'), 34),\n",
       " (('brown', 'stock'), 30),\n",
       " (('thin', 'slice'), 29),\n",
       " (('olive', 'oil'), 27),\n",
       " (('little', 'piece'), 27),\n",
       " (('small', 'piece'), 27),\n",
       " (('low', 'fire'), 25),\n",
       " (('put', 'saucepan'), 25),\n",
       " (('chopped', 'fine'), 25),\n",
       " (('half', 'ounce'), 25),\n",
       " (('serve', 'hot'), 22),\n",
       " (('yolk', 'egg'), 22),\n",
       " (('boiling', 'water'), 22)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "phrases = Counter(ngrams(tokens_lemmatized,2))\n",
    "\n",
    "phrases.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "I82p3I3VFLLd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('season', 'salt', 'pepper'), 44),\n",
       " (('bread', 'crumb', 'ground'), 13),\n",
       " (('cut', 'thin', 'slice'), 13),\n",
       " (('taste', 'lemon', 'peel'), 12),\n",
       " (('pinch', 'grated', 'cheese'), 11),\n",
       " (('good', 'olive', 'oil'), 10),\n",
       " (('small', 'piece', 'butter'), 10),\n",
       " (('saucepan', 'piece', 'butter'), 9),\n",
       " (('another', 'piece', 'butter'), 9),\n",
       " (('cut', 'little', 'piece'), 9),\n",
       " (('crumb', 'ground', 'fine'), 9),\n",
       " (('cut', 'small', 'piece'), 9),\n",
       " (('half', 'inch', 'thick'), 9),\n",
       " (('greased', 'butter', 'sprinkled'), 9),\n",
       " (('medium', 'sized', 'onion'), 9),\n",
       " (('ounce', 'sweet', 'almond'), 9),\n",
       " (('tomato', 'sauce', '12'), 8),\n",
       " (('little', 'piece', 'butter'), 8),\n",
       " (('three', 'half', 'ounce'), 8),\n",
       " (('fire', 'piece', 'butter'), 7)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = Counter(ngrams(tokens_lemmatized,3))\n",
    "phrases.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdSc16_YnkXB"
   },
   "source": [
    "# **spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "AFhPB-pGnvJO"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy # for visualization\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwCipESGtdI9"
   },
   "source": [
    "## **Parts of Speech with spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "FtH1pscPo-mA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He PRON | pronoun\n",
      "drinks VERB | verb\n",
      "a DET | determiner\n",
      "drink NOUN | noun\n"
     ]
    }
   ],
   "source": [
    "s1 = nlp(\"He drinks a drink\")\n",
    "\n",
    "for word in s1:\n",
    "    print(word.text, word.pos_,\"|\",spacy.explain(word.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "p27YaQ33pYk2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON pronoun PRP\n",
      "fish VERB verb VBP\n",
      "a DET determiner DT\n",
      "fish NOUN noun NN\n"
     ]
    }
   ],
   "source": [
    "s2 = nlp(\"I fish a fish\")\n",
    "\n",
    "for word in s2:\n",
    "    print(word.text,word.pos_, spacy.explain(word.pos_),word.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "XU2G5HRupmIN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verb, non-3rd person singular present'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('VBP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSFJyk85psLQ"
   },
   "source": [
    "## **Syntactic Dependency**\n",
    "\n",
    "- It helps us to know the relation between tokens\n",
    "\n",
    "- How each word is connected and dependent on each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "vOgSAVJApwbT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kiara', 'NNP', 'PROPN', 'nsubj')\n",
      "('likes', 'VBZ', 'VERB', 'ROOT')\n",
      "('Sid', 'NNP', 'PROPN', 'dobj')\n"
     ]
    }
   ],
   "source": [
    "s3 = nlp(\"Kiara likes Sid\")\n",
    "\n",
    "for word in s3:\n",
    "    print((word.text, word.tag_, word.pos_, word.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "hJIvh162p6nO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'spacy' from 'C:\\\\Users\\\\ronak\\\\anaconda3\\\\lib\\\\site-packages\\\\spacy\\\\__init__.py'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "m19xQMAZqAu2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"bba3ce8ac67243b39061ea50c6df82de-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Kiara</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">likes</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Sid</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bba3ce8ac67243b39061ea50c6df82de-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bba3ce8ac67243b39061ea50c6df82de-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bba3ce8ac67243b39061ea50c6df82de-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bba3ce8ac67243b39061ea50c6df82de-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(s3, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWJnVVCbsnWH"
   },
   "source": [
    "## **Lemmatization with spaCy**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "uY2F-claqrXM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token=> studying Lemma=> study VERB\n",
      "Token=> student Lemma=> student NOUN\n",
      "Token=> study Lemma=> study NOUN\n",
      "Token=> studies Lemma=> study NOUN\n",
      "Token=> studio Lemma=> studio NOUN\n",
      "Token=> studious Lemma=> studious ADJ\n"
     ]
    }
   ],
   "source": [
    "doc_lemma = nlp(\"studying student study studies studio studious\")\n",
    "\n",
    "for word in doc_lemma:\n",
    "    print(\"Token=>\", word.text, \"Lemma=>\", word.lemma_, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-> good lemma-> good ADJ\n",
      "Token-> goods lemma-> good NOUN\n",
      "Token-> run lemma-> run VERB\n",
      "Token-> running lemma-> run VERB\n",
      "Token-> runner lemma-> runner NOUN\n",
      "Token-> was lemma-> be AUX\n",
      "Token-> be lemma-> be AUX\n",
      "Token-> were lemma-> be AUX\n"
     ]
    }
   ],
   "source": [
    "doc_lemma=nlp(\"good goods run running runner was be were\")\n",
    "for word in doc_lemma:\n",
    "    print(\"Token->\",word.text,\"lemma->\",word.lemma_,word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fmgIgYNnof2"
   },
   "source": [
    "## **NER with spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "OUllK3DHnqi-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 DATE\n",
      "Orange NORP\n",
      "Turkey GPE\n",
      "Orange County GPE\n",
      "U.S. GPE\n",
      "Apple ORG\n",
      "2 billion dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "wikitext = nlp(u\"By 2020 the telecom company Orange, will relocate from Turkey to Orange County in the U.S. close to Apple.It will cost them 2 billion dollars.\")\n",
    "\n",
    "for entity in wikitext.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "-dUFwFBNqPWA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does GPE means\n",
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH0yVX8SoVwz"
   },
   "source": [
    "## **Entity Types**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "jIUBKmgLoZTz"
   },
   "outputs": [],
   "source": [
    "def explain_text_entities(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(f'{ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "PQLnFSIcobiK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla, Label: ORG, Companies, agencies, institutions, etc.\n",
      "20%, Label: PERCENT, Percentage, including \"%\"\n",
      "the months, Label: DATE, Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Tesla has gained 20% market share in the months since')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRDgY3kGq-Gg"
   },
   "source": [
    "## **Semantic Similarity**\n",
    "\n",
    "object1.similarity(object2)\n",
    "\n",
    "- Uses:\n",
    "\n",
    "- Recommendation systems\n",
    "\n",
    "- Data Preprocessing eg removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "zUMWASSwrHCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5523053837367063\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Similarity of object\n",
    "doc1 = nlp(\"India\")\n",
    "doc2 = nlp(\"India\")\n",
    "doc3 = nlp(\"Lion\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    "print(doc1.similarity(doc3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
